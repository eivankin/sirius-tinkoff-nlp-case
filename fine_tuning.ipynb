{
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8e25db09d9b64ae3b35dbe7e45cf50cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a859554f7934a0993b65a290ef667d2",
       "IPY_MODEL_5675a9d832764ddb8b5042d2f3b87142",
       "IPY_MODEL_dfe7c877e5254cdfa943b377b4376ce9"
      ],
      "layout": "IPY_MODEL_130805d8af924f9da7b04e8a3b06c256"
     }
    },
    "4a859554f7934a0993b65a290ef667d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e34f12b7e2e4fe4a62e80467dada1fc",
      "placeholder": "​",
      "style": "IPY_MODEL_41482967fae042ce8246350beee18360",
      "value": "100%"
     }
    },
    "5675a9d832764ddb8b5042d2f3b87142": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_183493d43fe54ec4ab5e4b343a1345b6",
      "max": 25811,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_751115f9f8b741c6959b7bfff409288c",
      "value": 25811
     }
    },
    "dfe7c877e5254cdfa943b377b4376ce9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7eca88dfddc947dfa73f2a01ad570fee",
      "placeholder": "​",
      "style": "IPY_MODEL_82a5e0ea76f242b5a9a9fa4b000b06d4",
      "value": " 25811/25811 [00:04&lt;00:00, 5922.01it/s]"
     }
    },
    "130805d8af924f9da7b04e8a3b06c256": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e34f12b7e2e4fe4a62e80467dada1fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41482967fae042ce8246350beee18360": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "183493d43fe54ec4ab5e4b343a1345b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "751115f9f8b741c6959b7bfff409288c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7eca88dfddc947dfa73f2a01ad570fee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82a5e0ea76f242b5a9a9fa4b000b06d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Подготовка данных\n",
    "Будем использовать скрипт `prepare_messages.py`, предоставленный в задании."
   ],
   "metadata": {
    "id": "1ij9rAl8qlUk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/solemn-leader/sirius-test"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhmQ0SP_p9-m",
    "outputId": "2a178f82-2e50-43a0-acc3-181c4a85d6d5"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Cloning into 'sirius-test'...\n\nremote: Enumerating objects: 27, done.\u001B[K\n\nremote: Counting objects: 100% (27/27), done.\u001B[K\n\nremote: Compressing objects: 100% (25/25), done.\u001B[K\n\nremote: Total 27 (delta 6), reused 1 (delta 0), pack-reused 0\u001B[K\n\nReceiving objects: 100% (27/27), 793.30 KiB | 6.10 MiB/s, done.\n\nResolving deltas: 100% (6/6), done.\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!cd chat_export && unzip result.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6hJTkI1sAPM",
    "outputId": "648fb414-9266-4357-e2dc-93cb3b5b69f0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Archive:  result.zip\n\n  inflating: result.json             \n"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python sirius-test/prepare_messages.py --tg-history-path 'chat_export/result.json' --output-path 'data.csv'"
   ],
   "metadata": {
    "id": "o4_k4q8frCsz"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим, что данные преобразованы корректно:"
   ],
   "metadata": {
    "id": "RRu_kB3RyWok"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Для запуска на Kaggle\n",
    "!cp /kaggle/input/scala-chat/data.csv data.csv"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:22:22.626539Z",
     "iopub.execute_input": "2023-09-07T21:22:22.627152Z",
     "iopub.status.idle": "2023-09-07T21:22:23.747122Z",
     "shell.execute_reply.started": "2023-09-07T21:22:22.627125Z",
     "shell.execute_reply": "2023-09-07T21:22:23.745714Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "hW6XtZqUvX3V",
    "outputId": "a1d6855c-870a-4691-d04b-87339d529c87",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:38:31.590884953Z",
     "start_time": "2023-09-07T20:38:31.133170076Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:22:29.123165Z",
     "iopub.execute_input": "2023-09-07T21:22:29.123565Z",
     "iopub.status.idle": "2023-09-07T21:22:29.683978Z",
     "shell.execute_reply.started": "2023-09-07T21:22:29.123532Z",
     "shell.execute_reply": "2023-09-07T21:22:29.682979Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "  context_3 context_2                                          context_1  \\\n0       NaN       NaN                                                NaN   \n1       NaN       NaN                            Не смог разгадать ребус   \n2       NaN       NaN                                                NaN   \n3       NaN       NaN  в группе все андрофонеры и ты позвал джей ли\\n...   \n4       NaN       NaN                                                NaN   \n\n                                            response  \n0                            Не смог разгадать ребус  \n1  Намек на то что мы не такие но вроде никто бун...  \n2  в группе все андрофонеры и ты позвал джей ли\\n...  \n3             еще чат офтопа\\nтут с телеками за 300?  \n4  все скатится в говно, если сюда зайдут решетка...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context_3</th>\n      <th>context_2</th>\n      <th>context_1</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Не смог разгадать ребус</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Не смог разгадать ребус</td>\n      <td>Намек на то что мы не такие но вроде никто бун...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>в группе все андрофонеры и ты позвал джей ли\\n...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>в группе все андрофонеры и ты позвал джей ли\\n...</td>\n      <td>еще чат офтопа\\nтут с телеками за 300?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>все скатится в говно, если сюда зайдут решетка...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejlLMpKvvmCj",
    "outputId": "a2466886-0c75-4894-b205-259c343d547f",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:27:32.911602606Z",
     "start_time": "2023-09-07T20:27:32.899103810Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pandas.core.frame.DataFrame'>\n\nRangeIndex: 25811 entries, 0 to 25810\n\nData columns (total 4 columns):\n\n #   Column     Non-Null Count  Dtype \n\n---  ------     --------------  ----- \n\n 0   context_3  4613 non-null   object\n\n 1   context_2  8219 non-null   object\n\n 2   context_1  16273 non-null  object\n\n 3   response   25740 non-null  object\n\ndtypes: object(4)\n\nmemory usage: 806.7+ KB\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning\n",
    "## Загрузка модели"
   ],
   "metadata": {
    "id": "PsPxRsX-sTB2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q 'transformers[torch]'"
   ],
   "metadata": {
    "id": "XTvJwd5msVvF",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:28:26.319146715Z",
     "start_time": "2023-09-07T20:28:08.029972604Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:22:35.253897Z",
     "iopub.execute_input": "2023-09-07T21:22:35.254369Z",
     "iopub.status.idle": "2023-09-07T21:22:51.328148Z",
     "shell.execute_reply.started": "2023-09-07T21:22:35.254327Z",
     "shell.execute_reply": "2023-09-07T21:22:51.326924Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import (AutoTokenizer, AutoModelWithLMHead,\n",
    "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer)"
   ],
   "metadata": {
    "id": "ZBqHdVKhxLGC",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:38:38.598322490Z",
     "start_time": "2023-09-07T20:38:34.467264702Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:22:59.275764Z",
     "iopub.execute_input": "2023-09-07T21:22:59.276126Z",
     "iopub.status.idle": "2023-09-07T21:23:13.970044Z",
     "shell.execute_reply.started": "2023-09-07T21:22:59.276094Z",
     "shell.execute_reply": "2023-09-07T21:23:13.967908Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Для запуска на Kaggle\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:23:13.971907Z",
     "iopub.execute_input": "2023-09-07T21:23:13.972938Z",
     "iopub.status.idle": "2023-09-07T21:23:13.977779Z",
     "shell.execute_reply.started": "2023-09-07T21:23:13.972901Z",
     "shell.execute_reply": "2023-09-07T21:23:13.976428Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
    "model = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZV6O9fMx7Ax",
    "outputId": "423e0832-d0e4-4428-9cf1-135244239035",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:38:45.162635830Z",
     "start_time": "2023-09-07T20:38:40.011654631Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:23:22.591675Z",
     "iopub.execute_input": "2023-09-07T21:23:22.592063Z",
     "iopub.status.idle": "2023-09-07T21:23:41.686046Z",
     "shell.execute_reply.started": "2023-09-07T21:23:22.592032Z",
     "shell.execute_reply": "2023-09-07T21:23:41.684984Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17603f04a3ca4ea9bacca65c8bb072b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26143187d0e0415da1f34106bbfadfd7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "827e21a72ef84703a7c8945af55576a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/107 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b2d5415aca4dbdb58dff418975ef7c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/379 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c7afac72574c198995c4c71846d550"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/874 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14649826fe66450bac1f42da76b47477"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/1.52G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75b65e3dd0a542d4b423817eab604563"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проверка работы модели"
   ],
   "metadata": {
    "id": "omlCrRAR0uxu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_response(model_output: str) -> str:\n",
    "    return re.sub(r'(@@[А-Я]+@@)+', '\\n>>> ', model_output) + '\\n'"
   ],
   "metadata": {
    "id": "JMbjSgm035D4",
    "execution": {
     "iopub.status.busy": "2023-09-07T21:24:16.236664Z",
     "iopub.execute_input": "2023-09-07T21:24:16.237249Z",
     "iopub.status.idle": "2023-09-07T21:24:16.243284Z",
     "shell.execute_reply.started": "2023-09-07T21:24:16.237208Z",
     "shell.execute_reply": "2023-09-07T21:24:16.241857Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_answers(question, model):\n",
    "    print('Question:', question)\n",
    "    prompt = f'@@ПЕРВЫЙ@@ {question} @@ВТОРОЙ@@'\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    generated_token_ids = model.generate(\n",
    "        **inputs,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        num_beams=3,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=1.2,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        eos_token_id=50261,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    context_with_response = [extract_response(tokenizer.decode(sample_token_ids)) for sample_token_ids in generated_token_ids]\n",
    "    print('Responses:', *context_with_response, sep='\\n')\n",
    "\n",
    "# Проверим на вопросе по будущей тематике бота\n",
    "get_answers('что такое scala?', model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx_dOiA50uUr",
    "outputId": "2558121b-a7eb-432b-d635-26b61b829a57",
    "execution": {
     "iopub.status.busy": "2023-09-07T21:27:56.838314Z",
     "iopub.execute_input": "2023-09-07T21:27:56.838674Z",
     "iopub.status.idle": "2023-09-07T21:28:21.727574Z",
     "shell.execute_reply.started": "2023-09-07T21:27:56.838645Z",
     "shell.execute_reply": "2023-09-07T21:28:21.726607Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": "Setting `pad_token_id` to `eos_token_id`:50261 for open-end generation.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Question: что такое scala?\nResponses:\nчто такое scala?\n>>> Scala - язык программирования. Scala это язык разметки, который используется в веб-разработке.\n>>> Спасибо большое!\n>>> Пожалуйста. Обращайся, если что. :) P.S. А что за сайт-то? :) И почему ты решил, что это программирование? :D Или это не так работает? О_о\n>>> Я не знаю, как это работает, я просто предположил. Ну и, по-моему, это именно то, чем я сейчас занимаюсь. :\n\nчто такое scala?\n>>> Scala - язык программирования. Scala это язык разметки, который используется в веб-разработке.\n>>> Спасибо большое!\n>>> Пожалуйста. Обращайся, если что. :) P.S. А что за сайт-то? :) И почему ты решил, что это программирование? :D Или это не так работает? О_о\n>>> Я не знаю, как это работает, я просто предположил. Ну и, по-моему, это именно то, чем я сейчас занимаюсь. =\n\nчто такое scala?\n>>> Scala - язык программирования. Scala это язык разметки, который используется в веб-разработке.\n>>> Спасибо большое!\n>>> Пожалуйста. Обращайся, если что. :) P.S. А что за сайт-то? :) И почему ты решил, что это программирование? :D Или это не так работает? О_о\n>>> Я не знаю, как это работает, я просто предположил. Ну и, по-моему, это именно то, чем я сейчас занимаюсь. Но\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Приведение данных к нужному формату"
   ],
   "metadata": {
    "id": "6cVTXBsSDyNZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "speakers_switch_tokens = (tokenizer.decode([50257]), tokenizer.decode([50258]))\n",
    "\n",
    "MAX_SAMPLES = 1_000  # На 25 тысяч записей не хватает терпения и бесплатных квот :)\n",
    "\n",
    "dataset = []\n",
    "for _, sentences in tqdm(df.sample(MAX_SAMPLES).iterrows(), total=MAX_SAMPLES):\n",
    "    with_speakers = [speakers_switch_tokens[i % 2] + s for i, s in enumerate(filter(lambda x: isinstance(x, str), sentences))]\n",
    "    dataset.append(''.join(with_speakers))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8e25db09d9b64ae3b35dbe7e45cf50cf",
      "4a859554f7934a0993b65a290ef667d2",
      "5675a9d832764ddb8b5042d2f3b87142",
      "dfe7c877e5254cdfa943b377b4376ce9",
      "130805d8af924f9da7b04e8a3b06c256",
      "9e34f12b7e2e4fe4a62e80467dada1fc",
      "41482967fae042ce8246350beee18360",
      "183493d43fe54ec4ab5e4b343a1345b6",
      "751115f9f8b741c6959b7bfff409288c",
      "7eca88dfddc947dfa73f2a01ad570fee",
      "82a5e0ea76f242b5a9a9fa4b000b06d4"
     ]
    },
    "id": "AdobIXdPD1vM",
    "outputId": "f8efee5f-a325-4836-ee87-e0c5f9ee3087",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:38:52.941597471Z",
     "start_time": "2023-09-07T20:38:52.820077278Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:28:44.660234Z",
     "iopub.execute_input": "2023-09-07T21:28:44.660609Z",
     "iopub.status.idle": "2023-09-07T21:28:44.772357Z",
     "shell.execute_reply.started": "2023-09-07T21:28:44.660578Z",
     "shell.execute_reply": "2023-09-07T21:28:44.771377Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b26ca8c911c04ee79e58e2ed3071427b"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b654MdasIJuo",
    "outputId": "412dd9e8-163c-4d5c-d3f6-eb09c38925d8",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:38:54.516920033Z",
     "start_time": "2023-09-07T20:38:54.511618591Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:28:46.685619Z",
     "iopub.execute_input": "2023-09-07T21:28:46.686829Z",
     "iopub.status.idle": "2023-09-07T21:28:46.694025Z",
     "shell.execute_reply.started": "2023-09-07T21:28:46.686788Z",
     "shell.execute_reply": "2023-09-07T21:28:46.693033Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'@@ПЕРВЫЙ@@будет код по сложности уровня питона, если не проще@@ВТОРОЙ@@О, так это вообще для меня отлично) нужно хоть с какого-то *овно кода начать, а то я только читаю, прихожу, и делаю свои задачи на питоне, хотя мне кажется скала без фп не такая уж и сложная, но это поверхностно'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Дообучение"
   ],
   "metadata": {
    "id": "DSYzjMp20y4M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_toks = tokenizer(dataset)"
   ],
   "metadata": {
    "id": "fiGk3zE9uJF7",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:06.161236698Z",
     "start_time": "2023-09-07T20:39:06.113402825Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:28:49.286601Z",
     "iopub.execute_input": "2023-09-07T21:28:49.287196Z",
     "iopub.status.idle": "2023-09-07T21:28:49.586797Z",
     "shell.execute_reply.started": "2023-09-07T21:28:49.287152Z",
     "shell.execute_reply": "2023-09-07T21:28:49.585804Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "total_max_len = max(map(lambda x: len(x), all_toks['input_ids']))\n",
    "total_max_len"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oZCiaCzuPvD",
    "outputId": "be03dc22-7105-4990-f683-94e9349a8efd",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:06.391270543Z",
     "start_time": "2023-09-07T20:39:06.387256066Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:28:50.864160Z",
     "iopub.execute_input": "2023-09-07T21:28:50.864609Z",
     "iopub.status.idle": "2023-09-07T21:28:50.878100Z",
     "shell.execute_reply.started": "2023-09-07T21:28:50.864571Z",
     "shell.execute_reply": "2023-09-07T21:28:50.876510Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "1247"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "total_max_len = 512  # Иначе не помещается в память GPU в Colab"
   ],
   "metadata": {
    "id": "xJlx7V-84IIX",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:07.964030418Z",
     "start_time": "2023-09-07T20:39:07.959739929Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:28:57.265059Z",
     "iopub.execute_input": "2023-09-07T21:28:57.265494Z",
     "iopub.status.idle": "2023-09-07T21:28:57.270824Z",
     "shell.execute_reply.started": "2023-09-07T21:28:57.265459Z",
     "shell.execute_reply": "2023-09-07T21:28:57.269777Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Код взят из класса LineByLineTextDataset\n",
    "batch_encoding = tokenizer(\n",
    "    [line for line in dataset if (len(line) > 0 and not line.isspace())],\n",
    "    add_special_tokens=True, truncation=True, max_length=total_max_len\n",
    ")\n",
    "tokenized_dataset = batch_encoding[\"input_ids\"]\n",
    "tokenized_dataset = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in tokenized_dataset]"
   ],
   "metadata": {
    "id": "NGUvrLChYZ3T",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:09.112270628Z",
     "start_time": "2023-09-07T20:39:09.062490348Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:00.421404Z",
     "iopub.execute_input": "2023-09-07T21:29:00.421767Z",
     "iopub.status.idle": "2023-09-07T21:29:00.561615Z",
     "shell.execute_reply.started": "2023-09-07T21:29:00.421738Z",
     "shell.execute_reply": "2023-09-07T21:29:00.560659Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_dataset[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIRNePNYP6Zi",
    "outputId": "5591ffce-a6e6-402d-adc9-6e8ca8c170b6",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:09.547644956Z",
     "start_time": "2023-09-07T20:39:09.545225864Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:02.688986Z",
     "iopub.execute_input": "2023-09-07T21:29:02.689656Z",
     "iopub.status.idle": "2023-09-07T21:29:02.698538Z",
     "shell.execute_reply.started": "2023-09-07T21:29:02.689623Z",
     "shell.execute_reply": "2023-09-07T21:29:02.697376Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "execution_count": 21,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': tensor([50257, 50168,  6429,   334, 11035,  4999,  1001,  5097,    16,   827,\n           322,  8622, 50258,   630,    16,   453,   481,  2141,   541,   703,\n          8046,    13,  1513,  2838,   281,  5608,    17,   299,  2925,   291,\n           315, 32531,  6622,    16,   367,   502,   417,   687, 30597,    16,\n         11992,  4285,    16,   289, 14257,  1617,  8266,   309,  1001, 17610,\n            16,  1830,   784,  3073,   830,   455,   749,   469,   293,   322,\n          3433,  1588,   289, 25899,    16,   515,   481, 19404,  3214])}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ],
   "metadata": {
    "id": "-kyvDmIczm9C",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:11.198251667Z",
     "start_time": "2023-09-07T20:39:11.194514460Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:04.385903Z",
     "iopub.execute_input": "2023-09-07T21:29:04.386585Z",
     "iopub.status.idle": "2023-09-07T21:29:04.390946Z",
     "shell.execute_reply.started": "2023-09-07T21:29:04.386550Z",
     "shell.execute_reply": "2023-09-07T21:29:04.389984Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=500,\n",
    "    logging_steps=100\n",
    ")"
   ],
   "metadata": {
    "id": "cccd8xuxzv6z",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:11.551759724Z",
     "start_time": "2023-09-07T20:39:11.522720070Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:11.712094Z",
     "iopub.execute_input": "2023-09-07T21:29:11.713150Z",
     "iopub.status.idle": "2023-09-07T21:29:11.798363Z",
     "shell.execute_reply.started": "2023-09-07T21:29:11.713116Z",
     "shell.execute_reply": "2023-09-07T21:29:11.797389Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "text": "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=tokenized_dataset)"
   ],
   "metadata": {
    "id": "mSx-5C1C0MsD",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:15.347466720Z",
     "start_time": "2023-09-07T20:39:12.235559830Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:21.016945Z",
     "iopub.execute_input": "2023-09-07T21:29:21.017331Z",
     "iopub.status.idle": "2023-09-07T21:29:26.995381Z",
     "shell.execute_reply.started": "2023-09-07T21:29:21.017301Z",
     "shell.execute_reply": "2023-09-07T21:29:26.994342Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TaKhi7ex2BAt",
    "outputId": "0194a2e5-0169-4412-e89f-b32b7ba6fd7b",
    "ExecuteTime": {
     "end_time": "2023-09-07T20:39:17.677669371Z",
     "start_time": "2023-09-07T20:39:15.474033155Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-07T21:29:27.080285Z",
     "iopub.execute_input": "2023-09-07T21:29:27.080591Z",
     "iopub.status.idle": "2023-09-07T21:34:46.479572Z",
     "shell.execute_reply.started": "2023-09-07T21:29:27.080566Z",
     "shell.execute_reply": "2023-09-07T21:34:46.478573Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "text": "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 05:12, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>4.845900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.609800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.610000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.457600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.429000</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "execution_count": 25,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=500, training_loss=4.590443115234375, metrics={'train_runtime': 319.0976, 'train_samples_per_second': 3.134, 'train_steps_per_second': 1.567, 'total_flos': 145809636777984.0, 'train_loss': 4.590443115234375, 'epoch': 1.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model('fine_tuned_model')"
   ],
   "metadata": {
    "id": "zrFP-ZM32Em3",
    "execution": {
     "iopub.status.busy": "2023-09-07T21:34:46.485689Z",
     "iopub.execute_input": "2023-09-07T21:34:46.492314Z",
     "iopub.status.idle": "2023-09-07T21:34:51.377761Z",
     "shell.execute_reply.started": "2023-09-07T21:34:46.492261Z",
     "shell.execute_reply": "2023-09-07T21:34:51.376696Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Проверка работы дообученной модели"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "new_model = AutoModelWithLMHead.from_pretrained('fine_tuned_model')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:35:27.280130Z",
     "iopub.execute_input": "2023-09-07T21:35:27.280502Z",
     "iopub.status.idle": "2023-09-07T21:35:32.817072Z",
     "shell.execute_reply.started": "2023-09-07T21:35:27.280474Z",
     "shell.execute_reply": "2023-09-07T21:35:32.815948Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1479: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "get_answers('что такое scala?', new_model)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:36:58.538824Z",
     "iopub.execute_input": "2023-09-07T21:36:58.539530Z",
     "iopub.status.idle": "2023-09-07T21:37:23.677631Z",
     "shell.execute_reply.started": "2023-09-07T21:36:58.539496Z",
     "shell.execute_reply": "2023-09-07T21:37:23.676655Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "text": "Setting `pad_token_id` to `eos_token_id`:50261 for open-end generation.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Question: что такое scala?\nResponses:\nчто такое scala?\n>>> scala - это язык программирования, а не язык для работы с массивами.\nВ общем, если вы не программист, то вам не понять, что такое Scala.\n>>> Ну так я и не хочу программистом быть, я хочу работать с массивом, но не понимаю, как это сделать.\n>>> Я тоже не знаю, поэтому и спрашиваю у вас, может есть какие-то книги на эту тему, которые вам помогут разобраться в этой ситуации. Спасибо! :)\nНа самом деле\n\nчто такое scala?\n>>> scala - это язык программирования, а не язык для работы с массивами.\nВ общем, если вы не программист, то вам не понять, что такое Scala.\n>>> Ну так я и не хочу программистом быть, я хочу работать с массивом, но не понимаю, как это сделать.\n>>> Я тоже не знаю, поэтому и спрашиваю у вас, может есть какие-то книги на эту тему, которые вам помогут разобраться в этой ситуации. Спасибо! :)\nНапример,\n\nчто такое scala?\n>>> scala - это язык программирования, а не язык для работы с массивами.\nВ общем, если вы не программист, то вам не понять, что такое Scala.\n>>> Ну так я и не хочу программистом быть, я хочу работать с массивом, но не понимаю, как это сделать.\n>>> Я тоже не знаю, поэтому и спрашиваю у вас, может есть какие-то книги на эту тему, которые вам помогут разобраться в этой ситуации. Спасибо! :)\nНапишите потом\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r saved_model.zip fine_tuned_model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:45:32.768701Z",
     "iopub.execute_input": "2023-09-07T21:45:32.769089Z",
     "iopub.status.idle": "2023-09-07T21:46:51.814365Z",
     "shell.execute_reply.started": "2023-09-07T21:45:32.769056Z",
     "shell.execute_reply": "2023-09-07T21:46:51.813135Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  adding: fine_tuned_model/ (stored 0%)\n  adding: fine_tuned_model/pytorch_model.bin (deflated 7%)\n  adding: fine_tuned_model/generation_config.json (deflated 24%)\n  adding: fine_tuned_model/training_args.bin (deflated 49%)\n  adding: fine_tuned_model/config.json (deflated 51%)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'saved_model.zip')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-07T21:46:51.817727Z",
     "iopub.execute_input": "2023-09-07T21:46:51.818130Z",
     "iopub.status.idle": "2023-09-07T21:46:51.826933Z",
     "shell.execute_reply.started": "2023-09-07T21:46:51.818099Z",
     "shell.execute_reply": "2023-09-07T21:46:51.825923Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": [
    {
     "execution_count": 33,
     "output_type": "execute_result",
     "data": {
      "text/plain": "/kaggle/working/saved_model.zip",
      "text/html": "<a href='saved_model.zip' target='_blank'>saved_model.zip</a><br>"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
